{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":78156,"sourceType":"datasetVersion","datasetId":44109},{"sourceId":12725403,"sourceType":"datasetVersion","datasetId":8041590}],"dockerImageVersionId":31090,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import os, cv2, random\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.model_selection import train_test_split\nimport tensorflow as tf\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator\nfrom tensorflow.keras.layers import Input, Conv2D, MaxPooling2D, Flatten, Dense, Dropout, BatchNormalization, Activation, GlobalAveragePooling2D\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau\nimport pandas as pd\nfrom tensorflow.keras import layers, Model\nfrom tensorflow.keras.applications import ResNet50, VGG16, EfficientNetB0\nfrom tensorflow.keras.callbacks import EarlyStopping","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-08-10T12:10:47.350044Z","iopub.execute_input":"2025-08-10T12:10:47.350334Z","iopub.status.idle":"2025-08-10T12:11:00.654927Z","shell.execute_reply.started":"2025-08-10T12:10:47.350311Z","shell.execute_reply":"2025-08-10T12:11:00.654340Z"}},"outputs":[{"name":"stderr","text":"2025-08-10 12:10:49.943433: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1754827850.123420      36 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1754827850.174921      36 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"}],"execution_count":4},{"cell_type":"code","source":"DATASET_DIR = \"/kaggle/input/utkface-new/UTKFace\"\n\nfile_list = [\n    os.path.join(DATASET_DIR, f)\n    for f in os.listdir(DATASET_DIR)\n    if f.lower().endswith(\".jpg\")\n]\n\nrandom_file = random.choice(file_list)\nparts = os.path.basename(random_file).split(\"_\")\nage = int(parts[0])\ngender = \"Male\" if int(parts[1]) == 0 else \"Female\"\n\nimg = cv2.imread(random_file)\nimg = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n\nplt.imshow(img)\nplt.title(f\"Age: {age}, Gender: {gender}\")\nplt.axis('off')\nplt.show()\n\nprint(f\"File: {os.path.basename(random_file)}\")\nprint(f\"Age: {age}\")\nprint(f\"Gender: {gender}\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"male_count = 0\nfemale_count = 0\n\nfor filename in os.listdir(DATASET_DIR):\n    if filename.endswith(\".jpg\"):\n        try:\n            gender = int(filename.split(\"_\")[1])\n            if gender == 0:\n                male_count += 1\n            elif gender == 1:\n                female_count += 1\n        except:\n            continue\nprint(f\"T·ªïng s·ªë ·∫£nh: {male_count + female_count}\")\nprint(f\"S·ªë nam   : {male_count}\")\nprint(f\"S·ªë n·ªØ    : {female_count}\")\n\nlabels = ['Male', 'Female']\ncounts = [male_count, female_count]\n\nplt.bar(labels, counts, color=['blue', 'pink'])\nplt.title(\"Gender Distribution in UTKFace\")\nplt.ylabel(\"S·ªë l∆∞·ª£ng ·∫£nh\")\nplt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-08T09:34:24.317826Z","iopub.execute_input":"2025-08-08T09:34:24.318092Z","iopub.status.idle":"2025-08-08T09:34:24.458913Z","shell.execute_reply.started":"2025-08-08T09:34:24.318073Z","shell.execute_reply":"2025-08-08T09:34:24.458256Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"ages = []\n\nfor filename in os.listdir(DATASET_DIR):\n    if filename.endswith(\".jpg\"):\n        try:\n            age = int(filename.split(\"_\")[0])\n            ages.append(age)\n        except:\n            continue  # b·ªè qua file l·ªói\n\nplt.figure(figsize=(10, 5))\nplt.hist(ages, bins=range(0, 101, 5), color='skyblue', edgecolor='black')\nplt.title(\"Ph√¢n b·ªë tu·ªïi trong UTKFace\")\nplt.xlabel(\"Tu·ªïi\")\nplt.ylabel(\"S·ªë l∆∞·ª£ng ·∫£nh\")\nplt.grid(True)\nplt.xticks(range(0, 101, 5))\nplt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-08T09:34:32.033860Z","iopub.execute_input":"2025-08-08T09:34:32.034121Z","iopub.status.idle":"2025-08-08T09:34:32.292165Z","shell.execute_reply.started":"2025-08-08T09:34:32.034102Z","shell.execute_reply":"2025-08-08T09:34:32.291497Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"train_files, temp_files = train_test_split(file_list, test_size=0.3, random_state=42)\n\nval_files, test_files = train_test_split(temp_files, test_size=0.5, random_state=42)\n\nprint(f\"T·ªïng ·∫£nh     : {len(file_list)}\")\nprint(f\"Train ·∫£nh    : {len(train_files)}\")\nprint(f\"Validation ·∫£nh: {len(val_files)}\")\nprint(f\"Test ·∫£nh     : {len(test_files)}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-08T09:37:07.361444Z","iopub.execute_input":"2025-08-08T09:37:07.361734Z","iopub.status.idle":"2025-08-08T09:37:07.382152Z","shell.execute_reply.started":"2025-08-08T09:37:07.361714Z","shell.execute_reply":"2025-08-08T09:37:07.381432Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"MAX_AGE = 116\n\ndef load_and_preprocess(file_name, img_size=128):\n    file_path = os.path.join(DATASET_DIR, file_name)\n    img = cv2.imread(file_path)\n    if img is None:\n        raise ValueError(f\"Kh√¥ng ƒë·ªçc ƒë∆∞·ª£c file ·∫£nh: {file_path}\")\n    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)  # ƒë·ªïi BGR sang RGB\n    img = cv2.resize(img, (img_size, img_size))\n    img = img.astype(np.float32)\n    return img\n\n\ndef extract_labels_from_filename(file_path):\n    # filename c√≥ d·∫°ng: age_gender_race_date.jpg.chip.jpg\n    base = os.path.basename(file_path)\n    parts = base.split('_')\n    age = int(parts[0])\n    gender = int(parts[1])  # 0 = Male, 1 = Female\n    age = age / MAX_AGE\n    return gender, age","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-08T09:34:36.886176Z","iopub.execute_input":"2025-08-08T09:34:36.886773Z","iopub.status.idle":"2025-08-08T09:34:36.891993Z","shell.execute_reply.started":"2025-08-08T09:34:36.886747Z","shell.execute_reply":"2025-08-08T09:34:36.891197Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from tensorflow.keras.preprocessing.image import ImageDataGenerator\n\ndef create_datagen(augment=False):\n    if augment:\n        return ImageDataGenerator(\n            rescale=1./255,\n            rotation_range=20,\n            width_shift_range=0.1,\n            height_shift_range=0.1,\n            horizontal_flip=True,\n            zoom_range=0.1,\n            fill_mode='nearest'\n        )\n    else:\n        return ImageDataGenerator(rescale=1./255)\n\ntrain_datagen = create_datagen(augment=True)\nval_datagen = create_datagen(augment=False)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-08T09:36:04.417101Z","iopub.execute_input":"2025-08-08T09:36:04.417372Z","iopub.status.idle":"2025-08-08T09:36:04.422115Z","shell.execute_reply.started":"2025-08-08T09:36:04.417353Z","shell.execute_reply":"2025-08-08T09:36:04.421503Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def multi_output_generator(file_list, datagen, batch_size=32, img_size=128, shuffle=True):\n    while True:\n        if shuffle:\n            random.shuffle(file_list)\n        for i in range(0, len(file_list), batch_size):\n            batch_files = file_list[i:i+batch_size]\n            batch_images = []\n            gender_labels = []\n            age_labels = []\n\n            for file in batch_files:\n                img = load_and_preprocess(file, img_size)\n                gender, age = extract_labels_from_filename(file)\n                batch_images.append(img)\n                gender_labels.append(gender)\n                age_labels.append(age)\n\n            batch_images = np.array(batch_images)\n\n            batch_images_aug_iter = datagen.flow(batch_images, batch_size=len(batch_images), shuffle=False)\n            batch_images_aug = next(batch_images_aug_iter)\n\n            yield batch_images_aug, {\n                'gender_output': np.array(gender_labels),\n                'age_output': np.array(age_labels, dtype=np.float32)\n            }","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-08T09:36:06.403476Z","iopub.execute_input":"2025-08-08T09:36:06.404182Z","iopub.status.idle":"2025-08-08T09:36:06.409466Z","shell.execute_reply.started":"2025-08-08T09:36:06.404157Z","shell.execute_reply":"2025-08-08T09:36:06.408698Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import tensorflow as tf\n\nMAX_AGE = 116\n\ndef extract_labels_from_filename(file_path):\n    base = tf.strings.split(file_path, os.sep)[-1]\n    parts = tf.strings.split(base, '_')\n    age = tf.strings.to_number(parts[0], out_type=tf.float32) / MAX_AGE\n    gender = tf.strings.to_number(parts[1], out_type=tf.float32)\n    return gender, age\n\ndef load_and_preprocess(file_path, img_size=128):\n    img = tf.io.read_file(file_path)\n    img = tf.image.decode_jpeg(img, channels=3)\n    img = tf.image.resize(img, [img_size, img_size])\n    img = tf.cast(img, tf.float32) / 255.0\n    gender, age = extract_labels_from_filename(file_path)\n    return img, {'gender_output': gender, 'age_output': age}\n\ndef augment(img, labels):\n    img = tf.image.random_flip_left_right(img)\n    img = tf.image.random_brightness(img, max_delta=0.1)\n    img = tf.image.random_contrast(img, lower=0.9, upper=1.1)\n    img = tf.image.random_saturation(img, lower=0.9, upper=1.1)\n    img = tf.image.random_hue(img, max_delta=0.02)\n    return img, labels\n\ndef create_dataset(file_list, batch_size=64, augment_data=False, shuffle=True):\n    ds = tf.data.Dataset.from_tensor_slices(file_list)\n    if shuffle:\n        ds = ds.shuffle(buffer_size=len(file_list))\n    ds = ds.map(lambda x: load_and_preprocess(x, 128), num_parallel_calls=tf.data.AUTOTUNE)\n    if augment_data:\n        ds = ds.map(augment, num_parallel_calls=tf.data.AUTOTUNE)\n    ds = ds.batch(batch_size).prefetch(tf.data.AUTOTUNE)\n    return ds\n\n# Thay th·∫ø train_gen / val_gen\ntrain_gen = create_dataset(train_files, batch_size=64, augment_data=True, shuffle=True)\nval_gen = create_dataset(val_files, batch_size=64, augment_data=False, shuffle=False)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# train_gen = multi_output_generator(train_files, train_datagen, batch_size=32, img_size=128, shuffle=True)\n# val_gen = multi_output_generator(val_files, val_datagen, batch_size=32, img_size=128, shuffle=False)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-08T09:37:16.281666Z","iopub.execute_input":"2025-08-08T09:37:16.281929Z","iopub.status.idle":"2025-08-08T09:37:16.286187Z","shell.execute_reply.started":"2025-08-08T09:37:16.281910Z","shell.execute_reply":"2025-08-08T09:37:16.285505Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def build_model(base_model_class, pooling_mode='avg'):\n    if pooling_mode == 'flatten':\n        base_model = base_model_class(\n            include_top=False,\n            input_shape=(128, 128, 3),\n            weights='imagenet'\n        )\n        x = layers.Flatten()(base_model.output)\n    else:\n        base_model = base_model_class(\n            include_top=False,\n            input_shape=(128, 128, 3),\n            pooling=pooling_mode,\n            weights='imagenet'\n        )\n        x = base_model.output\n\n    x = layers.Dense(128, activation='relu')(x)\n    x = layers.Dropout(0.3)(x)\n\n    gender_output = layers.Dense(1, activation='sigmoid', name='gender_output')(x)\n    age_output = layers.Dense(1, activation='linear', name='age_output')(x)\n\n    model = Model(inputs=base_model.input, outputs=[gender_output, age_output])\n    model.compile(\n        optimizer='adam',\n        loss={\n            'gender_output': 'binary_crossentropy',\n            'age_output': 'mse'\n        },\n        metrics={\n            'gender_output': 'accuracy',\n            'age_output': 'mae'\n        }\n    )\n    return model\n\ndef train_and_save(model, name, train_gen, val_gen, output_dir=\"/kaggle/working\"):\n    early_stop = EarlyStopping(\n        monitor='val_loss',\n        patience=29,\n        restore_best_weights=True\n    )\n\n    history = model.fit(\n        train_gen,\n        validation_data=val_gen,\n        epochs=30,\n        callbacks=[early_stop],\n        verbose=1\n    )\n\n    pd.DataFrame(history.history).to_csv(f\"{output_dir}/{name}_history.csv\", index=False)\n    model.save(f\"{output_dir}/{name}_model.h5\")\n    print(f\"‚úÖ {name} ƒë√£ l∆∞u model & history.\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(\"===== Training ResNet50 =====\")\nmodel_resnet = build_model(ResNet50, pooling_mode='avg')\ntrain_and_save(model_resnet, \"ResNet50\", train_gen, val_gen)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(\"===== Training VGG16 =====\")\nmodel_vgg = build_model(VGG16, pooling_mode='flatten')\ntrain_and_save(model_vgg, \"VGG16\", train_gen, val_gen)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(\"===== Training EfficientNetB0 =====\")\nmodel_eff = build_model(EfficientNetB0, pooling_mode='avg')\ntrain_and_save(model_eff, \"EfficientNetB0\", train_gen, val_gen)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import pandas as pd\nimport matplotlib.pyplot as plt\nimport os\n\n# File paths\nfiles = {\n    \"EfficientNetB0\": \"/kaggle/input/my-results/EfficientNetB0_history.csv\",\n    \"ResNet50\": \"/kaggle/input/my-results/ResNet50_history.csv\",\n    \"VGG16\": \"/kaggle/input/my-results/VGG16_history.csv\"\n}\n\n# Th∆∞ m·ª•c l∆∞u ·∫£nh\nsave_dir = \"./plots\"\nos.makedirs(save_dir, exist_ok=True)\n\nfor name, path in files.items():\n    df = pd.read_csv(path)\n    \n    # ---- Plot Accuracy ----\n    plt.figure(figsize=(8, 5))\n    plt.plot(df['gender_output_accuracy'], label='Train Acc')\n    plt.plot(df['val_gender_output_accuracy'], linestyle='--', label='Val Acc')\n    plt.title(f'{name} - Gender Accuracy')\n    plt.xlabel('Epoch')\n    plt.ylabel('Accuracy')\n    plt.legend()\n    plt.grid(True)\n    plt.tight_layout()\n    plt.savefig(os.path.join(save_dir, f\"{name}_accuracy.png\"))\n    plt.close()\n    \n    # ---- Plot Loss ----\n    plt.figure(figsize=(8, 5))\n    plt.plot(df['loss'], label='Train Loss')\n    plt.plot(df['val_loss'], linestyle='--', label='Val Loss')\n    plt.title(f'{name} - Overall Loss')\n    plt.xlabel('Epoch')\n    plt.ylabel('Loss')\n    plt.legend()\n    plt.grid(True)\n    plt.tight_layout()\n    plt.savefig(os.path.join(save_dir, f\"{name}_loss.png\"))\n    plt.close()\n\nprint(f\"‚úÖ ƒê√£ l∆∞u bi·ªÉu ƒë·ªì v√†o th∆∞ m·ª•c: {save_dir}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-10T12:04:38.275168Z","iopub.execute_input":"2025-08-10T12:04:38.275440Z","iopub.status.idle":"2025-08-10T12:04:39.353686Z","shell.execute_reply.started":"2025-08-10T12:04:38.275416Z","shell.execute_reply":"2025-08-10T12:04:39.352908Z"}},"outputs":[{"name":"stdout","text":"‚úÖ ƒê√£ l∆∞u bi·ªÉu ƒë·ªì v√†o th∆∞ m·ª•c: ./plots\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"import pandas as pd\nimport matplotlib.pyplot as plt\nimport os\n\nfiles = {\n    \"EfficientNetB0\": \"/kaggle/input/my-results/EfficientNetB0_history.csv\",\n    \"ResNet50\": \"/kaggle/input/my-results/ResNet50_history.csv\",\n    \"VGG16\": \"/kaggle/input/my-results/VGG16_history.csv\"\n}\n\nsave_dir = \"./plots_compare\"\nos.makedirs(save_dir, exist_ok=True)\n\nhistories = {name: pd.read_csv(path) for name, path in files.items()}\n\n# ==== Train Accuracy ====\nplt.figure(figsize=(8, 5))\nfor name, df in histories.items():\n    plt.plot(df['gender_output_accuracy'], label=name)\nplt.title(\"Train Accuracy (Gender)\")\nplt.xlabel(\"Epoch\")\nplt.ylabel(\"Accuracy\")\nplt.legend()\nplt.grid(True)\nplt.tight_layout()\nplt.savefig(os.path.join(save_dir, \"train_accuracy.png\"))\nplt.close()\n\n# ==== Val Accuracy ====\nplt.figure(figsize=(8, 5))\nfor name, df in histories.items():\n    plt.plot(df['val_gender_output_accuracy'], label=name)\nplt.title(\"Validation Accuracy (Gender)\")\nplt.xlabel(\"Epoch\")\nplt.ylabel(\"Accuracy\")\nplt.legend()\nplt.grid(True)\nplt.tight_layout()\nplt.savefig(os.path.join(save_dir, \"val_accuracy.png\"))\nplt.close()\n\n# ==== Train Loss ====\nplt.figure(figsize=(8, 5))\nfor name, df in histories.items():\n    plt.plot(df['loss'], label=name)\nplt.title(\"Train Loss (Overall)\")\nplt.xlabel(\"Epoch\")\nplt.ylabel(\"Loss\")\nplt.legend()\nplt.grid(True)\nplt.tight_layout()\nplt.savefig(os.path.join(save_dir, \"train_loss.png\"))\nplt.close()\n\n# ==== Val Loss ====\nplt.figure(figsize=(8, 5))\nfor name, df in histories.items():\n    plt.plot(df['val_loss'], label=name)\nplt.title(\"Validation Loss (Overall)\")\nplt.xlabel(\"Epoch\")\nplt.ylabel(\"Loss\")\nplt.legend()\nplt.grid(True)\nplt.tight_layout()\nplt.savefig(os.path.join(save_dir, \"val_loss.png\"))\nplt.close()\n\nprint(f\"‚úÖ ƒê√£ l∆∞u c√°c bi·ªÉu ƒë·ªì so s√°nh v√†o th∆∞ m·ª•c: {save_dir}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-10T12:06:14.628607Z","iopub.execute_input":"2025-08-10T12:06:14.628894Z","iopub.status.idle":"2025-08-10T12:06:15.393470Z","shell.execute_reply.started":"2025-08-10T12:06:14.628874Z","shell.execute_reply":"2025-08-10T12:06:15.392586Z"}},"outputs":[{"name":"stdout","text":"‚úÖ ƒê√£ l∆∞u c√°c bi·ªÉu ƒë·ªì so s√°nh v√†o th∆∞ m·ª•c: ./plots_compare\n","output_type":"stream"}],"execution_count":3},{"cell_type":"code","source":"MAX_AGE = 116 \n\nDATASET_DIR = \"/kaggle/input/utkface-new/UTKFace\"\nmodel_paths = {\n    \"EfficientNetB0\": \"/kaggle/input/my-results/EfficientNetB0_model.h5\",\n    \"ResNet50\": \"/kaggle/input/my-results/ResNet50_model.h5\",\n    \"VGG16\": \"/kaggle/input/my-results/VGG16_model.h5\"\n}\n\ntest_files = [\n    os.path.join(DATASET_DIR, f)\n    for f in os.listdir(DATASET_DIR)\n    if f.lower().endswith(\".jpg\")\n]\n\ntest_gen = create_dataset(test_files, batch_size=64, augment_data=False, shuffle=False)\n\ncustom_objects = {\n    \"mse\": tf.keras.losses.MeanSquaredError(),\n    \"mae\": tf.keras.losses.MeanAbsoluteError()\n}\n\nfor name, model_path in model_paths.items():\n    print(f\"üîç ƒêang ƒë√°nh gi√° {name}...\")\n    model = tf.keras.models.load_model(model_path, custom_objects=custom_objects)\n    results = model.evaluate(test_gen, verbose=1)\n    print(\"Test loss and metrics:\", results)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-10T12:30:23.356365Z","iopub.execute_input":"2025-08-10T12:30:23.356603Z","iopub.status.idle":"2025-08-10T12:32:07.794933Z","shell.execute_reply.started":"2025-08-10T12:30:23.356587Z","shell.execute_reply":"2025-08-10T12:32:07.794328Z"}},"outputs":[{"name":"stdout","text":"üîç ƒêang ƒë√°nh gi√° EfficientNetB0...\n\u001b[1m371/371\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 34ms/step - age_output_loss: 0.0243 - age_output_mean_absolute_error: 0.1164 - gender_output_accuracy: 0.7966 - gender_output_loss: 0.4385 - loss: 0.4628\nTest loss and metrics: [0.4640033543109894, 0.4398691952228546, 0.024005640298128128, 0.11607392877340317, 0.7962291240692139]\nüîç ƒêang ƒë√°nh gi√° ResNet50...\n\u001b[1m371/371\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m30s\u001b[0m 62ms/step - age_output_loss: 0.0100 - age_output_mean_absolute_error: 0.0784 - gender_output_accuracy: 0.9414 - gender_output_loss: 0.1486 - loss: 0.1586\nTest loss and metrics: [0.16278740763664246, 0.15308408439159393, 0.00978559534996748, 0.07761602848768234, 0.9399358630180359]\nüîç ƒêang ƒë√°nh gi√° VGG16...\n\u001b[1m371/371\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m48s\u001b[0m 125ms/step - age_output_loss: 0.0171 - age_output_mean_absolute_error: 0.0980 - gender_output_accuracy: 0.9114 - gender_output_loss: 0.2074 - loss: 0.2245\nTest loss and metrics: [0.22372928261756897, 0.20685997605323792, 0.016798432916402817, 0.09734412282705307, 0.9119706153869629]\n","output_type":"stream"}],"execution_count":18}]}